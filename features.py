# features.py

import csv
import pandas as pd
# ğŸ”¥ ä¿®æ­£: os, traceback ã‚’ãƒ•ã‚¡ã‚¤ãƒ«å†’é ­ã«ç§»å‹•
import os
import traceback
from PySide6.QtCore import QObject, Signal, QRunnable, Slot, QCoreApplication, QThread, QTimer
from PySide6.QtWidgets import QApplication
from concurrent.futures import ThreadPoolExecutor
import time
import re
import math
from decimal import Decimal, ROUND_DOWN, ROUND_HALF_UP, ROUND_UP


#==============================================================================
# 1. éåŒæœŸå‡¦ç†ç®¡ç†ã‚¯ãƒ©ã‚¹
#==============================================================================
class Worker(QRunnable):
    """å®Ÿè¡Œå¯èƒ½ãªãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰"""
    def __init__(self, fn, *args, **kwargs):
        super(Worker, self).__init__()
        self.fn = fn
        self.args = args
        self.kwargs = kwargs
        # self.signals = kwargs.get('signals') # signalsã¯ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã®ã§å‰Šé™¤å¯


    @Slot()
    def run(self):
        try:
            self.fn(*self.args, **self.kwargs)
        except Exception as e:
            error_info = traceback.format_exc()
            print(f"Worker thread error:\n{error_info}")
            # Workerã‚¯ãƒ©ã‚¹è‡ªä½“ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ã‚·ã‚°ãƒŠãƒ«ã‚’ç™ºè¡Œã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã ãŒã€
            # AsyncDataManagerã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã«ä»»ã›ã‚‹
            # if self.signals and hasattr(self.signals, 'error_occurred'):
            #     self.signals.error_occurred.emit(f"ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{e}")


class AsyncDataManager(QObject):
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã—ã€UIã®å¿œç­”æ€§ã‚’ç¶­æŒã™ã‚‹"""
    data_ready = Signal(pd.DataFrame)
    task_progress = Signal(str, int, int) # main_qt._update_progress_dialogã«æ¥ç¶š
    search_results_ready = Signal(list)
    analysis_results_ready = Signal(str)
    replace_from_file_completed = Signal(list, str)
    product_discount_completed = Signal(list, str)

    # UIã¸ã®å®‰å…¨ãªé€šçŸ¥ã‚·ã‚°ãƒŠãƒ«
    close_progress_requested = Signal()
    status_message_requested = Signal(str, int, bool)
    show_welcome_requested = Signal()
    cleanup_backend_requested = Signal() # æ–°è¦è¿½åŠ : ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è¦æ±‚ã‚·ã‚°ãƒŠãƒ«

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ç”¨ã®æ–°ã—ã„ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚·ã‚°ãƒŠãƒ«
    # main_qtã«ç›´æ¥æ¥ç¶šã™ã‚‹ï¼ˆAsyncDataManagerãŒemitã—ã€main_qtãŒLoadingOverlayã‚’åˆ¶å¾¡ï¼‰
    file_loading_started = Signal()
    file_loading_progress = Signal(str, int, int)
    file_loading_finished = Signal()
    
    def __init__(self, app_instance):
        super().__init__()
        self.app = app_instance
        self.executor = ThreadPoolExecutor(max_workers=1)
        self.current_load_mode = 'normal'
        self.backend_instance = None
        self.is_cancelled = False
        self.current_task = None

        # AsyncDataManagerè‡ªèº«ã®UIé€šçŸ¥ã‚·ã‚°ãƒŠãƒ«
        # ã“ã‚Œã‚‰ã®ã‚·ã‚°ãƒŠãƒ«ã¯ã€AsyncDataManagerãŒç›´æ¥ç®¡ç†ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºï¼ˆQProgressDialogï¼‰ã‚„
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€ã‚¦ã‚§ãƒ«ã‚«ãƒ ç”»é¢è¡¨ç¤ºã«æ¥ç¶šã•ã‚Œã‚‹
        self.close_progress_requested.connect(self.app._close_progress_dialog)
        self.status_message_requested.connect(self.app.show_operation_status)
        self.show_welcome_requested.connect(self.app.view_controller.show_welcome_screen)
        self.cleanup_backend_requested.connect(self.app._cleanup_backend) # æ–°è¦è¿½åŠ 

        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢é€£ã®ã‚·ã‚°ãƒŠãƒ«ã¯main_qtã«ç›´æ¥æ¥ç¶šã™ã‚‹ï¼ˆLoadingOverlayã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ï¼‰
        self.file_loading_started.connect(self.app.file_loading_started)
        self.file_loading_progress.connect(self.app.file_loading_progress)
        self.file_loading_finished.connect(self.app.file_loading_finished)
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä¿è­·
        self.timeout_timer = QTimer()
        self.timeout_timer.setSingleShot(True)
        self.timeout_timer.timeout.connect(self._handle_timeout)
        
    def cancel_current_task(self):
        """ç¾åœ¨ã®éåŒæœŸã‚¿ã‚¹ã‚¯ã«ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã‚’è¦æ±‚ã™ã‚‹"""
        self.is_cancelled = True
        if self.backend_instance:
            self.backend_instance.cancelled = True
        if self.current_task and isinstance(self.current_task, (QThread, ProductDiscountTask)):
            if hasattr(self.current_task, 'cancelled'):
                self.current_task.cancelled = True
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¿ã‚¤ãƒãƒ¼ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚‰åœæ­¢
        if self.timeout_timer.isActive():
            self.timeout_timer.stop()

    def load_full_dataframe_async(self, filepath, encoding, load_mode):
        self.is_cancelled = False
        self.current_load_mode = load_mode # AsyncDataManagerãŒç¾åœ¨ã®ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã‚’ä¿æŒ

        # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã®é–‹å§‹ã‚·ã‚°ãƒŠãƒ«ã‚’emit
        self.file_loading_started.emit()

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹ï¼ˆ30ç§’ï¼‰
        self.timeout_timer.start(30000)
        
        # filepathã¨encodingã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã«ä¿å­˜ (ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã§å¿…è¦ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚)
        self.current_filepath = filepath
        self.current_encoding = encoding

        worker = Worker(self._do_load_full_df, filepath, encoding, load_mode)
        self.executor.submit(worker.run)
    
    def _handle_timeout(self):
        """èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®å‡¦ç†"""
        print("WARNING: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        self.cancel_current_task() # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿæ™‚ã¯ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        self.file_loading_finished.emit() # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”»é¢ã‚’é–‰ã˜ã‚‹
        self.status_message_requested.emit(
            "ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ã‚ˆã‚Šå¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰ã§å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚",
            5000, True
        )
        self.cleanup_backend_requested.emit() # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self.show_welcome_requested.emit()

    def _do_load_full_df(self, filepath, encoding, load_mode, **kwargs):
        from db_backend import SQLiteBackend
        from lazy_loader import LazyCSVLoader

        df = None
        try:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¿ã‚¤ãƒãƒ¼ã‚’åœæ­¢
            if self.timeout_timer.isActive():
                self.timeout_timer.stop()

            # ãƒ•ã‚¡ã‚¤ãƒ«IOã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‹ã‚‰å¼•ãç¶™ãŒã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèªã¯
            # ã“ã“ã§ã¯è¡Œã‚ãªã„ãŒã€é€²æ—é€šçŸ¥ã¯ã“ã“ã‹ã‚‰ç™ºè¡Œã™ã‚‹
            self.file_loading_progress.emit(
                "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...", 0, 100
            )

            if load_mode == 'sqlite':
                self.backend_instance = SQLiteBackend(self.app)
                # ğŸ”¥ è¿½åŠ : main_windowã«ã‚‚è¨­å®š
                self.app.db_backend = self.backend_instance
                self.backend_instance.cancelled = self.is_cancelled

                def progress_callback(status, current, total):
                    if self.is_cancelled:
                        self.backend_instance.cancelled = True
                        return False # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã‚’ä¼ãˆã‚‹
                    # AsyncDataManagerã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é€²æ—ã‚·ã‚°ãƒŠãƒ«ã«æ¥ç¶š
                    self.file_loading_progress.emit(status, current, total)
                    return True # ç¶šè¡Œ

                columns, total_rows = self.backend_instance.import_csv_with_progress(
                    filepath, encoding, progress_callback=progress_callback
                )

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‰ã˜ã‚‹ã‚·ã‚°ãƒŠãƒ«ã‚’ç¢ºå®Ÿã«emit
                self.file_loading_finished.emit()

                if self.is_cancelled or columns is None:
                    self.backend_instance.close()
                    self.backend_instance = None
                    self.status_message_requested.emit("èª­ã¿è¾¼ã¿ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚", 3000, False)
                    self.cleanup_backend_requested.emit() # ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ™‚ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    self.show_welcome_requested.emit()
                    return # ã“ã“ã§çµ‚äº†

                if columns is not None:
                    self.backend_instance.header = columns
                    self.backend_instance.total_rows = total_rows
                    # ğŸ”¥ ä¿®æ­£: file_io_controller â†’ file_controller
                    if hasattr(self.app, 'file_controller'): # å±æ€§ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
                        self.app.file_controller.file_loaded.emit(self.backend_instance, filepath, encoding)
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šfile_controllerãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç›´æ¥_on_file_loadedã‚’å‘¼ã¶
                        # ãŸã ã—ã€ã“ã‚Œã¯é€šå¸¸ç™ºç”Ÿã—ãªã„ã¯ãš
                        from PySide6.QtCore import QTimer
                        QTimer.singleShot(0, lambda: self.app._on_file_loaded(self.backend_instance, filepath, encoding))
                    return # ã“ã“ã§çµ‚äº†

            elif load_mode == 'lazy':
                self.backend_instance = LazyCSVLoader(filepath, encoding)
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‰ã˜ã‚‹ã‚·ã‚°ãƒŠãƒ«ã‚’ç¢ºå®Ÿã«emit
                self.file_loading_finished.emit()
                
                # ğŸ”¥ ä¿®æ­£: file_io_controller â†’ file_controller
                if hasattr(self.app, 'file_controller'): # å±æ€§ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
                    self.app.file_controller.file_loaded.emit(self.backend_instance, filepath, encoding)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    from PySide6.QtCore import QTimer
                    QTimer.singleShot(0, lambda: self.app._on_file_loaded(self.backend_instance, filepath, encoding))
                return # ã“ã“ã§çµ‚äº†

            else: # normal mode
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã®é€²æ—è¡¨ç¤ºã‚’æ”¹å–„
                self.file_loading_progress.emit("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿ä¸­...", 0, 100)
                
                chunks = []
                chunk_size = 10000 # 10,000è¡Œãšã¤èª­ã¿è¾¼ã¿
                
                try:
                    # æœ€åˆã«è¡Œæ•°ã‚’é«˜é€Ÿã‚«ã‚¦ãƒ³ãƒˆ
                    # _fast_line_countã®ã‚ˆã†ãªå¤–éƒ¨ã‚³ãƒãƒ³ãƒ‰ã¯features.pyã®ä¾å­˜é–¢ä¿‚ã‚’å¢—ã‚„ã•ãªã„ãŸã‚é¿ã‘ã‚‹
                    # ã“ã“ã§ã¯Pythonæ¨™æº–ã®sum(1 for _ in f)ã‚’ä½¿ç”¨
                    with open(filepath, 'r', encoding=encoding, errors='ignore') as f: # errors='ignore'ã‚’è¿½åŠ 
                        total_lines = sum(1 for _ in f) # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å«ã‚€
                        if total_lines > 0: # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤ããƒ‡ãƒ¼ã‚¿è¡Œæ•°
                            total_data_lines = total_lines - 1
                        else:
                            total_data_lines = 0

                    # ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿
                    # config.py ã‹ã‚‰ CSV_READ_OPTIONS ã‚’å‚ç…§ã™ã‚‹
                    read_options = self.app.file_controller.config.CSV_READ_OPTIONS.copy() # ğŸ”¥ ä¿®æ­£: file_io_controller â†’ file_controller
                    read_options['encoding'] = encoding

                    # æ¥½å¤©å¸‚å ´CSVã®ç‰¹æ®Šãªå‡¦ç† (file_io_controllerã‹ã‚‰ã‚‚ç§»è¡Œ)
                    try:
                        with open(filepath, 'r', encoding=encoding) as f_peek:
                            first_line = f_peek.readline()
                            if first_line.count(',') > 100:
                                if read_options.get('engine') != 'python':
                                    read_options['low_memory'] = False
                    except Exception as e_peek:
                        print(f"WARNING: ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­è¡Œèª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ (AsyncDataManager): {e_peek}")
                        pass
                        
                    reader = pd.read_csv(filepath, encoding=encoding, dtype=str,
                                        chunksize=chunk_size, on_bad_lines='skip', **read_options) # ğŸ”¥ ä¿®æ­£: errors â†’ on_bad_lines
                    
                    rows_read = 0
                    for i, chunk in enumerate(reader):
                        if self.is_cancelled:
                            break
                            
                        chunks.append(chunk.fillna('')) # NaNã‚’ç©ºæ–‡å­—åˆ—ã«å¤‰æ›
                        rows_read += len(chunk)
                        
                        # é€²æ—ã‚’æ­£ç¢ºã«è¨ˆç®—
                        if total_data_lines > 0:
                            progress = min(int((rows_read / total_data_lines) * 100), 99) # 99%ã¾ã§
                        else:
                            progress = 100 # ãƒ‡ãƒ¼ã‚¿è¡ŒãŒãªã„å ´åˆã‚‚100%ã«
                        self.file_loading_progress.emit(
                            f"ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿ä¸­... ({rows_read:,}/{total_data_lines:,}è¡Œ)", 
                            progress, 100
                        )
                    
                    if not self.is_cancelled:
                        df = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame(columns=self.app.table_model._headers) # ç©ºã®å ´åˆã®ãƒ˜ãƒƒãƒ€ãƒ¼è€ƒæ…®
                        self.file_loading_progress.emit("èª­ã¿è¾¼ã¿å®Œäº†", 100, 100)
                    
                except Exception as e_chunk:
                    # ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ãŒå¤±æ•—ã—ãŸå ´åˆã¯é€šå¸¸ã®èª­ã¿è¾¼ã¿ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    print(f"ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã€é€šå¸¸èª­ã¿è¾¼ã¿ã«åˆ‡ã‚Šæ›¿ãˆ (AsyncDataManager): {e_chunk}")
                    df = pd.read_csv(filepath, encoding=encoding, dtype=str, on_bad_lines='skip').fillna('') # ğŸ”¥ ä¿®æ­£: errors â†’ on_bad_lines
                    self.file_loading_progress.emit("èª­ã¿è¾¼ã¿å®Œäº†", 100, 100)
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‰ã˜ã‚‹ã‚·ã‚°ãƒŠãƒ«ã‚’ç¢ºå®Ÿã«emit
                self.file_loading_finished.emit()

                if not self.is_cancelled:
                    self.data_ready.emit(df if df is not None else pd.DataFrame())
                else: # normalãƒ¢ãƒ¼ãƒ‰ã§ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚ŒãŸå ´åˆ
                    self.status_message_requested.emit("èª­ã¿è¾¼ã¿ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚", 3000, False)
                    self.cleanup_backend_requested.emit() # ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ™‚ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    self.show_welcome_requested.emit()

        except Exception as e:
            error_message = f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
            print(f"ERROR in _do_load_full_df: {error_message}")
            traceback.print_exc()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å¿…ãšãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‰ã˜ã‚‹
            self.file_loading_finished.emit()
            
            self.task_progress.emit(f"ã‚¨ãƒ©ãƒ¼: {e}", 1, 1) # task_progressã¯å¾“æ¥ã®QProgressDialogå‘ã‘ã ãŒã€å¿µã®ãŸã‚
            self.status_message_requested.emit(error_message, 5000, True)
            self.cleanup_backend_requested.emit() # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.show_welcome_requested.emit()
            self.data_ready.emit(pd.DataFrame()) # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®DataFrameã‚’é€ä¿¡

    def search_data_async(self, settings: dict, current_load_mode: str, parent_child_data: dict, selected_rows: set):
        self.is_cancelled = False
        worker = Worker(self._do_search, settings, current_load_mode, parent_child_data, selected_rows)
        self.executor.submit(worker.run)

    def _do_search(self, settings: dict, current_load_mode: str, parent_child_data: dict, selected_rows: set, **kwargs):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã•ã‚Œã‚‹æ¤œç´¢å‡¦ç†ã€‚GUIã‚¢ã‚¯ã‚»ã‚¹ã¯è¡Œã‚ãªã„ã€‚"""
        search_term = settings["search_term"]
        target_columns = settings["target_columns"]
        is_case_sensitive = settings["is_case_sensitive"]
        is_regex = settings["is_regex"]
        in_selection_only = settings["in_selection_only"]
        
        results = [] # ã“ã®resultsã«æœ€çµ‚çš„ãª (row_idx, col_idx) ã‚’è¿½åŠ ã™ã‚‹
        
        try:
            self.task_progress.emit("æ¤œç´¢ä¸­...", 0, 0)

            if current_load_mode == 'sqlite':
                # ğŸ”¥ ä¿®æ­£: main_windowã®db_backendã‚’ç›´æ¥å‚ç…§
                db_backend = self.app.db_backend if hasattr(self.app, 'db_backend') and self.app.db_backend else self.backend_instance
                
                if db_backend and hasattr(db_backend, 'search'):
                    print(f"DEBUG: SQLiteæ¤œç´¢é–‹å§‹ - backend: {db_backend}")
                    
                    # db_backend.search ã¯æ—¢ã« (row_idx, col_idx) ã‚’è¿”ã™ã‚ˆã†ã«ä¿®æ­£æ¸ˆã¿ãªã®ã§ã€
                    # ãã®ã¾ã¾resultsã«ä»£å…¥ã¾ãŸã¯extendã™ã‚‹
                    raw_results_from_db = db_backend.search( # å¤‰æ•°åã‚’å¤‰æ›´
                        search_term, 
                        target_columns, 
                        is_case_sensitive, 
                        is_regex
                    )
                    print(f"DEBUG: SQLiteæ¤œç´¢çµæœ: {len(raw_results_from_db)}ä»¶")
                    
                    # db_backend.searchã‹ã‚‰ã®çµæœã¯æ—¢ã«(row_idx, col_idx)å½¢å¼ãªã®ã§ã€ãã®ã¾ã¾ä½¿ç”¨
                    results.extend(raw_results_from_db) # ç›´æ¥resultsã«è¿½åŠ 
                else:
                    print("ERROR: SQLiteãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    self.status_message_requested.emit("ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“", 5000, True)
                    self.search_results_ready.emit([])
                    self.task_progress.emit("æ¤œç´¢ã‚¨ãƒ©ãƒ¼", 1, 1)
                    return # ã“ã“ã§çµ‚äº†

            elif current_load_mode == 'lazy':
                if self.backend_instance:
                    total_rows = self.backend_instance.get_total_rows()
                    def progress_callback(current):
                        if self.is_cancelled:
                            self.backend_instance.cancelled = True
                        self.task_progress.emit("ãƒ•ã‚¡ã‚¤ãƒ«å†…ã‚’æ¤œç´¢ä¸­...", current, total_rows)
                    
                    lazy_results = self.backend_instance.search_in_file( # å¤‰æ•°åã‚’å¤‰æ›´
                        search_term, target_columns, is_case_sensitive, is_regex,
                        progress_callback=progress_callback
                    )
                    results.extend(lazy_results) # çµæœã‚’resultsã«è¿½åŠ 
            
            else: # normal mode (DataFrame in memory)
                df = self.app.table_model._dataframe
                if df is None or df.empty:
                    self.search_results_ready.emit([])
                    self.task_progress.emit("æ¤œç´¢å®Œäº†", 1, 1)
                    return

                pattern = re.compile(
                    search_term if is_regex else re.escape(search_term),
                    0 if is_case_sensitive else re.IGNORECASE
                )
                
                target_rows = list(range(df.shape[0]))
                
                if in_selection_only:
                    selected_row_indices = {idx.row() for idx in self.app.table_view.selectionModel().selectedIndexes()}
                    target_rows = sorted(list(selected_row_indices.intersection(target_rows)))
                
                headers = self.app.table_model._headers
                target_col_indices = {headers.index(name) for name in target_columns if name in headers}
                
                total_search_cells = len(target_rows) * len(target_col_indices)
                processed_cells = 0
                
                for row_idx in target_rows:
                    if self.is_cancelled:
                        self.task_progress.emit("æ¤œç´¢ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ", 1, 1)
                        self.search_results_ready.emit([])
                        return
                    
                    for col_idx in target_col_indices:
                        if col_idx < len(df.columns):
                            cell_value = df.iat[row_idx, col_idx]
                            if cell_value is not None and pattern.search(str(cell_value)):
                                results.append((row_idx, col_idx)) # normal modeã®çµæœã‚‚resultsã«è¿½åŠ 
                        
                        processed_cells += 1
                        if processed_cells % 1000 == 0:
                            self.task_progress.emit(
                                "ãƒ‡ãƒ¼ã‚¿å†…ã‚’æ¤œç´¢ä¸­...", 
                                processed_cells, 
                                total_search_cells
                            )
            
            self.task_progress.emit("æ¤œç´¢å®Œäº†", 1, 1)
            
        except re.error as e:
            if QApplication.instance():
                self.status_message_requested.emit(f"æ­£è¦è¡¨ç¾ã‚¨ãƒ©ãƒ¼: {e}", 5000, True)
            self.search_results_ready.emit([])
            return
        except Exception as e:
            print(f"Error during search: {traceback.format_exc()}")
            if QApplication.instance():
                self.status_message_requested.emit(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", 5000, True)
            self.search_results_ready.emit([])
            return
        
        self.search_results_ready.emit(results) # æœ€çµ‚çš„ãªresultsã‚’emit

    def analyze_parent_child_async(self, db_backend_instance, column_name, mode):
        self.is_cancelled = False
        worker = Worker(self._do_analyze_parent_child_in_db, db_backend_instance, column_name, mode)
        self.executor.submit(worker.run)

    def _do_analyze_parent_child_in_db(self, db_backend_instance, column_name, mode, **kwargs):
        def progress_callback(status, current, total):
            if self.is_cancelled:
                db_backend_instance.cancelled = True
            self.task_progress.emit(status, current, total)
            
        success, message, total_rows = self.app.parent_child_manager.analyze_relationships_in_db(
            db_backend_instance, column_name, mode,
            progress_callback=progress_callback
        )
        if success:
            self.analysis_results_ready.emit(self.app.parent_child_manager.get_groups_summary())
        else:
            self.analysis_results_ready.emit(f"åˆ†æã‚¨ãƒ©ãƒ¼: {message}")
    
    def replace_from_file_async(self, db_backend_instance, current_dataframe, params):
        self.is_cancelled = False
        worker = Worker(self._do_replace_from_file, db_backend_instance, current_dataframe, params)
        self.executor.submit(worker.run)

    def _do_replace_from_file(self, db_backend_instance, current_dataframe, params, **kwargs):
        changes = []
        status_message = ""
        
        try:
            # (ğŸ“‹ çµ±åˆæ”¹å–„æ¡ˆ - ã“ã“ã‹ã‚‰è¿½åŠ )
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¤œè¨¼
            required_params = ['lookup_filepath', 'lookup_file_encoding', 
                               'target_col', 'lookup_key_col', 'replace_val_col']
            missing_params = [p for p in required_params if p not in params]
            if missing_params:
                raise KeyError(f"å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸è¶³: {missing_params}")
            # (ğŸ“‹ çµ±åˆæ”¹å–„æ¡ˆ - ã“ã“ã¾ã§è¿½åŠ )

            self.task_progress.emit("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...", 0, 1)
            lookup_df = pd.read_csv(params['lookup_filepath'], encoding=params['lookup_file_encoding'], dtype=str, on_bad_lines='warn').fillna('')
            self.task_progress.emit("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿å®Œäº†", 1, 1)
            
            if db_backend_instance:
                def progress_callback(status, current, total):
                    self.task_progress.emit(status, current, total)

                success, temp_changes, updated_count = db_backend_instance.execute_replace_from_file_in_db(
                    params, 
                    progress_callback=progress_callback
                )
                if success:
                    status_message = f"ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ç½®æ›å®Œäº†: {updated_count}ä»¶ã®ã‚»ãƒ«ã‚’ç½®æ›ã—ã¾ã—ãŸã€‚"
                    self.replace_from_file_completed.emit([], status_message)
                else:
                    status_message = "ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ç½®æ›ã«å¤±æ•—ã—ã¾ã—ãŸ (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼)ã€‚"
                    self.replace_from_file_completed.emit([], status_message)
                return

            else:
                self.task_progress.emit("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ä¸­...", 0, 1)
                df_current_memory_temp = current_dataframe.copy()
                
                df_current_memory_temp['_merge_key'] = df_current_memory_temp[params['target_col']].astype(str).str.strip().str.lower()
                
                lookup_cols_for_merge = lookup_df[[params['lookup_key_col'], params['replace_val_col']]].copy()
                lookup_cols_for_merge['_merge_key'] = lookup_cols_for_merge[params['lookup_key_col']].astype(str).str.strip().str.lower()
                
                lookup_cols_for_merge.drop_duplicates(subset=['_merge_key'], inplace=True)

                new_value_col_name_in_merged_df = "temp_replaced_value_col"
                lookup_cols_for_merge.rename(columns={params['replace_val_col']: new_value_col_name_in_merged_df}, inplace=True)

                merged_df = df_current_memory_temp.merge(
                    lookup_cols_for_merge,
                    on='_merge_key',
                    how='left'
                )
                self.task_progress.emit("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸å®Œäº†", 1, 1)
                
                current_target_values = current_dataframe[params['target_col']].astype(str).fillna('')
                new_lookup_values = merged_df[new_value_col_name_in_merged_df].astype(str).fillna('')
                
                changed_mask = merged_df[new_value_col_name_in_merged_df].notna() & \
                               (current_target_values != new_lookup_values)
                
                changed_indices = current_dataframe.index[changed_mask]
                
                if changed_indices.empty:
                    status_message = "ç½®æ›å¯¾è±¡ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                    self.replace_from_file_completed.emit([], status_message)
                    return
                
                total_changes = len(changed_indices)
                self.task_progress.emit("å¤‰æ›´ãƒªã‚¹ãƒˆã‚’ä½œæˆä¸­...", 0, total_changes)
                for i, row_idx in enumerate(changed_indices):
                    old_value = current_dataframe.at[row_idx, params['target_col']]
                    new_value = merged_df.at[row_idx, new_value_col_name_in_merged_df]
                    changes.append({
                        'item': str(row_idx),
                        'column': params['target_col'],
                        'old': str(old_value),
                        'new': str(new_value)
                    })
                    if i % 1000 == 0:
                        self.task_progress.emit("å¤‰æ›´ãƒªã‚¹ãƒˆã‚’ä½œæˆä¸­...", i, total_changes)
                
                status_message = f"{len(changed_indices)}ä»¶ã®ã‚»ãƒ«ã‚’å‚ç…§ç½®æ›ã—ã¾ã—ãŸ"
                self.replace_from_file_completed.emit(changes, status_message)

        except Exception as e:
            error_info = traceback.format_exc()
            status_message = f"ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ç½®æ›ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n{error_info}"
            self.replace_from_file_completed.emit([], status_message)

    def product_discount_async(self, db_backend, table_model, params):
        """å•†å“åˆ¥å‰²å¼•é©ç”¨ã®éåŒæœŸå‡¦ç†ã‚’é–‹å§‹ã™ã‚‹"""
        if self.current_task and self.current_task.isRunning():
            self.cancel_current_task()
            time.sleep(0.1)
        
        self.is_cancelled = False

        self.current_task = ProductDiscountTask(db_backend, table_model, params)
        self.current_task.discount_completed.connect(self.product_discount_completed.emit)
        self.current_task.task_progress.connect(self.task_progress.emit)
        self.current_task.start()

class ProductDiscountTask(QThread):
    """å•†å“åˆ¥å‰²å¼•é©ç”¨ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã™ã‚‹QThreadãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯"""
    discount_completed = Signal(list, str)
    task_progress = Signal(str, int, int)
    
    def __init__(self, backend, table_model, params):
        super().__init__()
        self.backend = backend
        self.table_model = table_model
        self.params = params
        self.cancelled = False
        
    def run(self):
        try:
            changes, message = self._execute_discount_calculation()
            if self.cancelled:
                self.discount_completed.emit([], "å•†å“åˆ¥å‰²å¼•é©ç”¨ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
            else:
                self.discount_completed.emit(changes, message)
        except Exception as e:
            error_info = traceback.format_exc()
            error_msg = f"å•†å“åˆ¥å‰²å¼•é©ç”¨ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n{str(e)}\n{error_info}"
            print(f"ProductDiscountTask error:\n{error_msg}")
            self.discount_completed.emit([], error_msg)
            
    def _execute_discount_calculation(self):
        changes = []
        status_message = ""
        
        try:
            self.task_progress.emit("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...", 0, 100)
            
            # ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯paramsã‹ã‚‰å–å¾— (SearchWidgetã§æ—¢ã«æ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æƒ³å®š)
            discount_file_encoding = self.params.get('discount_file_encoding', 'utf-8') 
            
            discount_df = pd.read_csv(
                self.params['discount_filepath'],
                encoding=discount_file_encoding,
                dtype=str,
                na_filter=False,
                keep_default_na=False
            )
            self.task_progress.emit("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿å®Œäº†", 10, 100)

            if self.cancelled: return [], "ã‚­ãƒ£ãƒ³ã‚»ãƒ«"
            
            if self.params['ref_product_col'] not in discount_df.columns:
                return [], f"ã‚¨ãƒ©ãƒ¼: å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã«å•†å“ç•ªå·åˆ—'{self.params['ref_product_col']}'ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            
            if self.params['ref_discount_col'] not in discount_df.columns:
                return [], f"ã‚¨ãƒ©ãƒ¼: å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã«å‰²å¼•ç‡åˆ—'{self.params['ref_discount_col']}'ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            
            self.task_progress.emit("å‰²å¼•ç‡ã‚’è§£æä¸­...", 20, 100)
            
            discount_lookup = {}
            total_discount_rows = len(discount_df)
            for i, row in discount_df.iterrows():
                if self.cancelled: return [], "ã‚­ãƒ£ãƒ³ã‚»ãƒ«"
                
                product_id = str(row[self.params['ref_product_col']]).strip()
                discount_str = str(row[self.params['ref_discount_col']]).strip()
                
                discount_rate = self._parse_discount_rate(discount_str)
                if discount_rate is not None:
                    discount_lookup[product_id] = discount_rate
                
                if i % 1000 == 0:
                    self.task_progress.emit(f"å‰²å¼•ç‡ã‚’è§£æä¸­... ({i}/{total_discount_rows})", 20 + int(i/total_discount_rows * 20), 100)
            
            if not discount_lookup:
                return [], "ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªå‰²å¼•ç‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            self.task_progress.emit("å‰²å¼•ç‡è§£æå®Œäº†", 40, 100)
            
            self.task_progress.emit("é‡‘é¡ã‚’è¨ˆç®—ä¸­...", 50, 100)
            
            if self.backend:
                changes = self._process_with_backend(discount_lookup)
            else:
                changes = self._process_with_dataframe(discount_lookup)
            
            status_message = f"å•†å“åˆ¥å‰²å¼•é©ç”¨å®Œäº†: {len(changes)}ä»¶ã®ã‚»ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚"
            self.task_progress.emit("å®Œäº†", 100, 100)
            
            return changes, status_message
            
        except Exception as e:
            error_info = traceback.format_exc()
            error_msg = f"è¨ˆç®—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n{str(e)}\n{error_info}"
            return [], error_msg
            
    def _parse_discount_rate(self, discount_str):
        try:
            cleaned = discount_str.replace('%', '').replace('ï¼…', '').strip()
            
            if not cleaned:
                return None
            
            rate = Decimal(cleaned)
            
            if rate > 1:
                rate = rate / Decimal('100')
            
            if Decimal('0') <= rate <= Decimal('1'):
                return float(rate)
            else:
                print(f"WARNING: å‰²å¼•ç‡ãŒç¯„å›²å¤–ã§ã™: '{discount_str}' -> {rate}")
                return None
                
        except Exception:
            print(f"WARNING: å‰²å¼•ç‡ã®è§£æã«å¤±æ•—: '{discount_str}'")
            return None
            
    def _process_with_dataframe(self, discount_lookup):
        changes = []
        df = self.table_model._dataframe
        
        if df is None or df.empty:
            return []
            
        product_col = self.params['current_product_col']
        price_col = self.params['current_price_col']
        
        if product_col not in df.columns or price_col not in df.columns:
            return []
            
        total_rows = len(df)
        for idx, row_series in df.iterrows():
            if self.cancelled: return []
            
            product_id = str(row_series.get(product_col, '')).strip()
            
            if product_id in discount_lookup:
                try:
                    current_price_str = str(row_series.get(price_col, '')).strip()
                    current_price = self._parse_price(current_price_str)
                    
                    if current_price is None:
                        continue
                        
                    discount_rate = Decimal(str(discount_lookup[product_id]))
                    discounted_price_decimal = Decimal('1.0') - discount_rate # å‰²å¼•ç‡ã‚’ä¹—æ•°ã«å¤‰æ›
                    final_price_decimal = Decimal(str(current_price)) * discounted_price_decimal
                    
                    final_price = self._apply_rounding(float(final_price_decimal), self.params['round_mode'])
                    final_price_str = str(int(final_price))
                    
                    if current_price_str != final_price_str:
                        changes.append({
                            'item': str(idx),
                            'column': price_col,
                            'old': current_price_str,
                            'new': final_price_str
                        })
                        
                except Exception as e:
                    print(f"WARNING: è¡Œ{idx}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            if idx % 1000 == 0:
                self.task_progress.emit(f"é‡‘é¡ã‚’è¨ˆç®—ä¸­... ({idx}/{total_rows})", 50 + int(idx/total_rows * 40), 100)

        return changes
        
    def _process_with_backend(self, discount_lookup):
        changes = []
        if not self.backend:
            return []

        total_rows = self.backend.get_total_rows()
        self.task_progress.emit("DBãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ä¸­...", 50, 100)
        
        try:
            df_from_backend = self.backend.get_all_data()
            
            product_col = self.params['current_product_col']
            price_col = self.params['current_price_col']

            if product_col not in df_from_backend.columns or price_col not in df_from_backend.columns:
                print("WARNING: DBãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å‡¦ç†ã§åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return []
            
            for idx, row_series in df_from_backend.iterrows():
                if self.cancelled: return []
                
                product_id = str(row_series.get(product_col, '')).strip()
                
                if product_id in discount_lookup:
                    try:
                        current_price_str = str(row_series.get(price_col, '')).strip()
                        current_price = self._parse_price(current_price_str)
                        
                        if current_price is None:
                            continue
                            
                        discount_rate = Decimal(str(discount_lookup[product_id]))
                        discounted_price_decimal = Decimal('1.0') - discount_rate # å‰²å¼•ç‡ã‚’ä¹—æ•°ã«å¤‰æ›
                        final_price_decimal = Decimal(str(current_price)) * discounted_price_decimal
                        
                        final_price = self._apply_rounding(float(final_price_decimal), self.params['round_mode'])
                        final_price_str = str(int(final_price))
                        
                        if current_price_str != final_price_str:
                            changes.append({
                                'row_idx': idx,
                                'col_name': price_col,
                                'new_value': final_price_str,
                                'old_value': current_price_str # Undoã®ãŸã‚ã«æ—§å€¤ã‚‚ä¿å­˜
                            })
                            
                    except Exception as e:
                        print(f"WARNING: DBå‡¦ç†ä¸­ã®è¡Œ{idx}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                if idx % 1000 == 0:
                    self.task_progress.emit(f"DBãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ä¸­... ({idx}/{total_rows})", 50 + int(idx/total_rows * 40), 100)

            if changes:
                # ã“ã® changes ã¯ {row_idx, col_name, new_value, old_value} å½¢å¼ã€‚
                # Undoå±¥æ­´ã«è¿½åŠ ã™ã‚‹ãŸã‚ã« {item, column, old, new} å½¢å¼ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
                # ã—ã‹ã—ã€ã“ã“ã§ã¯DBã®æ›´æ–°ã®ã¿ã‚’è¡Œã„ã€Undoå±¥æ­´ã¸ã®è¿½åŠ ã¯ main_qt.py ã§è¡Œã†ã®ãŒé©åˆ‡ã€‚
                # main_qt.py (_on_product_discount_completed) ã§ changes ã‚’å—ã‘å–ã‚Šã€Undo Manager ã«è¿½åŠ ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
                self.backend.update_cells(changes)
                # layoutChanged.emit() ã¯ main_qt.py ã§_on_product_discount_completed ã®å¾Œã«å‘¼ã°ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯ä¸è¦ã€‚
                # self.table_model.layoutChanged.emit() 
                
        except Exception as e:
            print(f"ERROR: _process_with_backend failed: {e}")
            traceback.print_exc()
            return []

        return changes

    def _parse_price(self, price_str):
        try:
            cleaned = re.sub(r'[^\d.]', '', price_str)
            if not cleaned:
                return None
            return float(cleaned)
        except (ValueError, TypeError):
            return None
            
    def _apply_rounding(self, price, round_mode):
        decimal_price = Decimal(str(price))
        
        if round_mode == 'truncate':
            return float(decimal_price.quantize(Decimal('1'), rounding=ROUND_DOWN))
        elif round_mode == 'round':
            return float(decimal_price.quantize(Decimal('1'), rounding=ROUND_HALF_UP))
        elif round_mode == 'ceil':
            return float(decimal_price.quantize(Decimal('1'), rounding=ROUND_UP))
        else:
            return float(decimal_price.quantize(Decimal('1'), rounding=ROUND_DOWN))


    def get_backend_instance(self):
        return self.backend_instance

    def shutdown(self):
        self.executor.shutdown(wait=True)
        if self.backend_instance and hasattr(self.backend_instance, 'close'):
            self.backend_instance.close()

#==============================================================================
# 2. ãã®ä»–ã®æ©Ÿèƒ½ç®¡ç†ã‚¯ãƒ©ã‚¹
#==============================================================================
class UndoRedoManager:
    """æ“ä½œå±¥æ­´ã‚’ç®¡ç†ã—ã€ã‚¢ãƒ³ãƒ‰ã‚¥/ãƒªãƒ‰ã‚¥æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, app, max_history=50):
        self.app = app
        self.history = []
        self.current_index = -1
        self.max_history = max_history

    def add_action(self, action):
        if self.current_index < len(self.history) - 1:
            self.history = self.history[:self.current_index + 1]
        
        self.history.append(action)
        
        if len(self.history) > self.max_history:
            self.history.pop(0)
        
        self.current_index = len(self.history) - 1
        self.app.update_menu_states()

    def undo(self):
        if not self.can_undo(): return
        action = self.history[self.current_index]
        self.app.apply_action(action, is_undo=True)
        self.current_index -= 1
        self.app.update_menu_states()

    def redo(self):
        if not self.can_redo(): return
        self.current_index += 1
        action = self.history[self.current_index]
        self.app.apply_action(action, is_undo=False)
        self.app.update_menu_states()

    def can_undo(self):
        return self.current_index >= 0

    def can_redo(self):
        return self.current_index < len(self.history) - 1

    def clear(self):
        self.history.clear()
        self.current_index = -1
        if hasattr(self.app, 'update_menu_states'):
            self.app.update_menu_states()

class CSVFormatManager:
    """CSVå½¢å¼ã®åˆ¤å®šã¨ç®¡ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹ (ç¾åœ¨ã¯ä¸»ã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼)"""
    def __init__(self, app):
        self.app = app

class ClipboardManager:
    """ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰æ“ä½œã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    @staticmethod
    def copy_cells_to_clipboard(app, cells_data):
        pass

    @staticmethod
    def get_paste_data_from_clipboard(app, start_row_idx, start_col_idx):
        return []

class CellMergeManager:
    """ã‚»ãƒ«é€£çµæ©Ÿèƒ½ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, app):
        self.app = app
    
    def concatenate_cells_right(self, target_cell):
        return False, "æœªå®Ÿè£…"

    def concatenate_cells_left(self, target_cell):
        return False, "æœªå®Ÿè£…"

class ColumnMergeManager:
    """åˆ—é€£çµæ©Ÿèƒ½ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, app):
        self.app = app

class ParentChildManager(QObject):
    """
    åˆ—ã®å€¤ã«åŸºã¥ãè¦ªå­é–¢ä¿‚ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ (PySide6ç‰ˆ)
    """
    analysis_completed = Signal(str)
    analysis_error = Signal(str)

    def __init__(self, ):
        super().__init__()
        self.parent_child_data = {}
        self.current_group_column = None
        self.df = None
        self.db_backend = None

    def analyze_relationships(self, dataframe, column_name, mode='consecutive'):
        """è¦ªå­é–¢ä¿‚åˆ†æã®ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£ãƒ¼ï¼ˆãƒ¡ãƒ¢ãƒªå†…ï¼‰"""
        if mode == 'global':
            return self._analyze_global(dataframe, column_name)
        else:
            return self._analyze_consecutive(dataframe, column_name)

    def analyze_relationships_in_db(self, db_backend_instance, column_name, mode='consecutive', progress_callback=None):
        """è¦ªå­é–¢ä¿‚åˆ†æã®ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£ãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰"""
        if mode == 'global':
            return self._analyze_global_in_db(db_backend_instance, column_name, progress_callback)
        else:
            return self._analyze_consecutive_in_db(db_backend_instance, column_name, progress_callback)

    def _analyze_consecutive(self, dataframe, column_name):
        """é€£ç¶šã™ã‚‹åŒã˜å€¤ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã¨ã¿ãªã—ã¦è¦ªå­é–¢ä¿‚ã‚’åˆ†æ"""
        if dataframe is None or dataframe.empty or column_name not in dataframe.columns:
            msg = "ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ã€åˆ—åãŒä¸æ­£ã§ã™ã€‚"
            self.analysis_error.emit(msg)
            return False, msg, 0
        
        self.df = dataframe
        self.current_group_column = column_name
        self.parent_child_data.clear()

        is_new_group = self.df[column_name] != self.df[column_name].shift()
        group_ids = is_new_group.cumsum()
        group_row_numbers = self.df.groupby(group_ids).cumcount()

        for i in range(len(self.df)):
            row_idx = self.df.index[i]
            self.parent_child_data[row_idx] = {
                'group_id': group_ids.iloc[i],
                'is_parent': group_row_numbers.iloc[i] == 0,
                'group_value': str(self.df.at[row_idx, column_name]).strip(),
            }

        summary_msg = f"åˆ—ã€Œ{column_name}ã€ã§{group_ids.max()}å€‹ã®é€£ç¶šã‚°ãƒ«ãƒ¼ãƒ—ã‚’è­˜åˆ¥ã—ã¾ã—ãŸ"
        self.analysis_completed.emit(self.get_groups_summary())
        return True, summary_msg, len(dataframe)

    def _analyze_global(self, dataframe, column_name):
        """ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã§åŒã˜å€¤ã‚’æŒã¤ã‚‚ã®ã‚’ä¸€ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã¨ã—ã¦è¦ªå­é–¢ä¿‚ã‚’åˆ†æ"""
        if dataframe is None or dataframe.empty or column_name not in dataframe.columns:
            msg = "ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ã€åˆ—åãŒä¸æ­£ã§ã™ã€‚"
            self.analysis_error.emit(msg)
            return False, msg, 0

        self.df = dataframe
        self.current_group_column = column_name
        self.parent_child_data.clear()

        is_child_flags = dataframe[column_name].duplicated(keep='first')
        
        unique_values = dataframe[column_name].unique()
        value_to_group_id = {val: i+1 for i, val in enumerate(unique_values)}

        for i in range(len(dataframe)):
            row_idx = dataframe.index[i]
            value = str(dataframe.at[row_idx, column_name]).strip()
            self.parent_child_data[row_idx] = {
                'group_id': value_to_group_id.get(value),
                'is_parent': not is_child_flags.iloc[i],
                'group_value': value,
            }
        
        summary_msg = f"åˆ—ã€Œ{column_name}ã€ã§{len(unique_values)}å€‹ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è­˜åˆ¥ã—ã¾ã—ãŸ"
        self.analysis_completed.emit(self.get_groups_summary())
        return True, summary_msg, len(dataframe)

    def _analyze_consecutive_in_db(self, db_backend_instance, column_name, progress_callback=None):
        """DBå†…ã§é€£ç¶šã™ã‚‹åŒã˜å€¤ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã¨ã—ã¦è¦ªå­é–¢ä¿‚ã‚’åˆ†æ"""
        if not db_backend_instance or not hasattr(db_backend_instance, 'conn'):
            return False, "DBã‚¨ãƒ©ãƒ¼", 0
        
        self.db_backend = db_backend_instance
        self.current_group_column = column_name
        self.parent_child_data.clear()

        try:
            if progress_callback:
                progress_callback("é€£ç¶šã‚°ãƒ«ãƒ¼ãƒ—ã‚’åˆ†æä¸­...", 0, 1)

            query = f'SELECT ROW_NUMBER() OVER (ORDER BY rowid) - 1 AS row_idx, "{column_name}" FROM "{db_backend_instance.table_name}"'
            df_from_db = pd.read_sql_query(query, db_backend_instance.conn)
            
            self._analyze_consecutive(df_from_db, column_name)
            
            if progress_callback:
                progress_callback("åˆ†æå®Œäº†", 1, 1)

            return True, "é€£ç¶šã‚°ãƒ«ãƒ¼ãƒ—åˆ†æå®Œäº†", len(df_from_db)
        except Exception as e:
            return False, f"DBã‚¨ãƒ©ãƒ¼: {e}", 0

    def _analyze_global_in_db(self, db_backend_instance, column_name, progress_callback=None):
        """DBå†…ã§ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã§åŒã˜å€¤ã‚’æŒã¤ã‚‚ã®ã‚’ä¸€ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã¨ã—ã¦è¦ªå­é–¢ä¿‚ã‚’åˆ†æ"""
        if not db_backend_instance or not hasattr(db_backend_instance, 'conn'):
            return False, "DBã‚¨ãƒ©ãƒ¼", 0

        self.db_backend = db_backend_instance
        self.current_group_column = column_name
        self.parent_child_data.clear()
        
        try:
            if progress_callback: progress_callback("è¦ªãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç‰¹å®šä¸­...", 0, 1)
            parent_query = f'SELECT "{column_name}", MIN(rowid) FROM "{db_backend_instance.table_name}" GROUP BY "{column_name}"'
            cursor = self.db_backend.conn.cursor()
            cursor.execute(parent_query)
            parent_lookup = {row[0]: row[1] for row in cursor.fetchall()}
            if progress_callback: progress_callback("è¦ªãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç‰¹å®šå®Œäº†", 1, 1)

            total_rows = db_backend_instance.get_total_rows()
            if progress_callback: progress_callback("å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’åˆ†é¡ä¸­...", 0, total_rows)
            query = f'SELECT ROW_NUMBER() OVER (ORDER BY rowid) - 1 AS row_idx, "{column_name}", rowid FROM "{db_backend_instance.table_name}"'
            cursor.execute(query)

            processed_rows = 0
            while True:
                rows_chunk = cursor.fetchmany(10000)
                if not rows_chunk:
                    break
                
                for row_data in rows_chunk:
                    row_idx, value, current_rowid = row_data
                    is_parent = (parent_lookup.get(value) == current_rowid)
                    self.parent_child_data[row_idx] = {
                        'group_id': parent_lookup.get(value),
                        'is_parent': is_parent,
                        'group_value': str(value).strip() if value is not None else '',
                    }
                
                processed_rows += len(rows_chunk)
                if progress_callback:
                    progress_callback("å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’åˆ†é¡ä¸­...", processed_rows, total_rows)

            summary_msg = f"åˆ—ã€Œ{column_name}ã€ã§{len(parent_lookup)}å€‹ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è­˜åˆ¥ã—ã¾ã—ãŸ"
            self.analysis_completed.emit(self.get_groups_summary())
            return True, summary_msg, len(self.parent_child_data)
        except Exception as e:
            return False, f"DBã‚¨ãƒ©ãƒ¼: {e}", 0

    def get_parent_rows_indices(self):
        if not self.parent_child_data: return []
        return [idx for idx, data in self.parent_child_data.items() if data['is_parent']]
    
    def get_child_rows_indices(self):
        if not self.parent_child_data: return []
        return [idx for idx, data in self.parent_child_data.items() if not data['is_parent']]
    
    def get_groups_summary(self):
        if not self.parent_child_data:
            return "è¦ªå­é–¢ä¿‚ãŒåˆ†æã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        group_counts = {}
        for data in self.parent_child_data.values():
            group_id = data['group_id']
            if group_id not in group_counts:
                group_counts[group_id] = {'value': data['group_value'], 'count': 0}
            group_counts[group_id]['count'] += 1
        
        summary = f"ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æçµæœï¼ˆåŸºæº–åˆ—ï¼š{self.current_group_column}ï¼‰\n\n"
        for group_id, info in sorted(group_counts.items(), key=lambda item: str(item[0])):
            child_count = info['count'] - 1
            summary += f"ã‚°ãƒ«ãƒ¼ãƒ—{group_id}: ã€Œ{info['value']}ã€ (è¦ª1è¡Œ, å­{child_count}è¡Œ, è¨ˆ{info['count']}è¡Œ)\n"
        
        total_parents = len(self.get_parent_rows_indices())
        total_children = len(self.get_child_rows_indices())
        summary += f"\n---\nå…¨ä½“: è¦ª {total_parents}è¡Œ, å­ {total_children}è¡Œ"
        
        return summary